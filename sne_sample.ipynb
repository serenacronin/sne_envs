{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the SNe sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from astropy.table import Table, vstack\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import math\n",
    "import os.path\n",
    "sys.path.append(os.path.join(os.getenv('HOME'),'workspace','galbase'))\n",
    "import astropy.io.fits as pyfits\n",
    "from astropy.wcs import WCS\n",
    "from astropy import units as u\n",
    "from astropy.utils.data import get_pkg_data_filename\n",
    "from astropy.utils.console import ProgressBar\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from operator import itemgetter\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from reproject import reproject_interp\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Galbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galbase_table = Table.read('gal_base.fits')\n",
    "galbase_df = galbase_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the PGCNAME, OBJNAME, and TAGS columns. Convert their datatypes from bytes to strings and remove whitespace. Also grab INCL_DEG and T columns. Combine them to create one DataFrame that will be output as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galbase_info_dict =  { 'PGC'       : galbase_df['PGCNAME'].str.decode('utf-8').str.strip(),\n",
    "                       'HOST'      : galbase_df['OBJNAME'].str.decode('utf-8').str.strip(),\n",
    "                       'RA'        : galbase_df['RA_DEG'],\n",
    "                       'DEC'       : galbase_df['DEC_DEG'],\n",
    "                       'INCL'      : galbase_df['INCL_DEG'],\n",
    "                       'PA'        : galbase_df['POSANG_DEG'],\n",
    "                       'R25'       : galbase_df['R25_DEG'],\n",
    "                       'DIST_MPC'  : galbase_df['DIST_MPC'],\n",
    "                       'T'         : galbase_df['T'],\n",
    "                       'VEL_REC'   : galbase_df['VHEL_KMS'],\n",
    "                       'TAGS'      : galbase_df['TAGS'].str.decode('utf-8').str.strip()\n",
    "    \n",
    "}\n",
    "\n",
    "galbase_info = pd.DataFrame(galbase_info_dict)\n",
    "galbase_info.to_csv('samples/galbase_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Open Supernova Catalog\n",
    "\n",
    "- Cut all entries without RAs and DECs\n",
    "    - Convert the RAs and DECs into degrees and average them\n",
    "- Cut entries without dates\n",
    "- Only keep types Ia, II, IIP, and Ibc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of entries without a discovery date\n",
    "full_cat = pd.read_csv('osc_full.csv')\n",
    "full_cat['Disc. Date'].isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the full catalog and immediately drop those without RA (assuming no RA = no Dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cat = pd.read_csv('osc_full.csv')\n",
    "full_cat = full_cat[full_cat['R.A.'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that will convert RAs and Decs to degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ra_conversion(ra_time):\n",
    "    \"\"\" Input:  RA in hrs:mins:secs\n",
    "        Output: RA in degrees\n",
    "    \"\"\"\n",
    "    ra_time = ra_time\n",
    "    ra_time = ra_time.split(':')\n",
    "    hours   = float(ra_time[0])\n",
    "    mins    = float(ra_time[1])\n",
    "    \n",
    "    # some already have the secs converted to decimal\n",
    "    try:\n",
    "        secs = float(ra_time[2])\n",
    "    except:\n",
    "        secs = 0.0\n",
    "    \n",
    "    time_in_hours = hours + mins/60.0 + secs/3600.0\n",
    "    ra_in_degrees = time_in_hours*15.0\n",
    "    \n",
    "    return(ra_in_degrees)\n",
    "        \n",
    "\n",
    "def dec_conversion(dec_time):\n",
    "    \"\"\" Input:  DEC in degree:arcmin:arcsec\n",
    "        Output: DEC in degrees\n",
    "    \"\"\"\n",
    "    dec_time  = dec_time.split(':')\n",
    "    degs      = float(dec_time[0])\n",
    "    \n",
    "    # apparently some are already in decimal degrees\n",
    "    try:\n",
    "        arcmins = float(dec_time[1])\n",
    "    except:\n",
    "        arcmins = 0.0\n",
    "    \n",
    "    # some already have the arcsecs converted to decimal\n",
    "    try:\n",
    "        arcsecs = float(dec_time[2])\n",
    "    except:\n",
    "        arcsecs = 0.0\n",
    "    \n",
    "                             # handle +/- degrees\n",
    "    deg_in_decimal_degrees = abs(degs) + (arcmins/60.0) + (arcsecs/3600.0)\n",
    "    \n",
    "    # make the degrees negative if they were negative before\n",
    "    if degs < 0:\n",
    "        deg_in_decimal_degrees = np.negative(deg_in_decimal_degrees)\n",
    "    \n",
    "    return(deg_in_decimal_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will:\n",
    "- get rid of all entries without a discovery date\n",
    "- only keep year for discovery date\n",
    "- get rid of all entries without an RA or Dec\n",
    "- average RAs and Decs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_catalog(full_catalog):\n",
    "    \"\"\" Input: full_catalog (the entire OSC catalog)\n",
    "        Output: names, dates, mmaxs, hosts, ras, decs,\n",
    "        zs, and types of each SN.\n",
    "        - exclude all entries without a date\n",
    "        - average RAs and Decs and convert to degrees\n",
    "    \"\"\"\n",
    "    # create lists that will store the info\n",
    "    names     = []\n",
    "    dates     = []\n",
    "    mmaxs     = []\n",
    "    hosts     = []\n",
    "    ras_deg   = []\n",
    "    ras_diff  = []\n",
    "    decs_diff = []\n",
    "    decs_deg  = []\n",
    "    zs        = []\n",
    "    types     = []\n",
    "    phots     = []\n",
    "    specs     = []\n",
    "    \n",
    "    \n",
    "    # loop over every single entry in the catalog; make certain cuts and clean up ras and decs\n",
    "    bar = ProgressBar(len(full_catalog), ipython_widget=True)    \n",
    "    for index, row in full_catalog.iterrows():\n",
    "        name       = row['Name']\n",
    "        orig_date  = row['Disc. Date']\n",
    "        mmax       = row['mmax']\n",
    "        host       = str(row['Host Name']).replace(\" \", \"\") # get rid of whitespace to xmatch with galbase\n",
    "        ra         = row['R.A.']\n",
    "        dec        = row['Dec.']\n",
    "        z          = row['z']\n",
    "        tipe       = row['Type']\n",
    "        phot       = row['Phot.']\n",
    "        spec       = row['Spec.']\n",
    "        \n",
    "        bar.update()\n",
    "        \n",
    "        # get rid of entries that have no date (get rid of remnants)\n",
    "        if pd.isnull(orig_date):\n",
    "            continue\n",
    "\n",
    "        # get dates into just year\n",
    "        date_year = (orig_date.split('/'))[0]\n",
    "\n",
    "        # grab the RAs and Decs; will need to fix up the RAs due to values labeled 0\n",
    "        old_ras  = np.array(str(row['R.A.']).split(','))\n",
    "        decs     = np.array(str(row['Dec.']).split(','))\n",
    "        \n",
    "        # get rid of ra entries that are just 0\n",
    "        ras = np.array([val for val in old_ras if val != '00:00:00.000'])\n",
    "        \n",
    "        # convert ras and decs to degrees\n",
    "        ras_deg_list  = [ra_conversion(ra) for ra in ras]\n",
    "        decs_deg_list = [dec_conversion(dec) for dec in decs]\n",
    "        \n",
    "        # take the median \n",
    "        ra_deg  = np.median(ras_deg_list)\n",
    "        dec_deg = np.median(decs_deg_list)\n",
    "    \n",
    "        # add column with difference between max coords and min coords (in degrees)\n",
    "        # this is to see which entries may have very different coordinate values from different sources\n",
    "        try:\n",
    "            ra_diff  = max(ras_deg_list) - min(ras_deg_list)\n",
    "            dec_diff = max(decs_deg_list) - min(decs_deg_list)\n",
    "        except ValueError:\n",
    "            ra_diff = 0\n",
    "            dec_dif = 0\n",
    "\n",
    "        # collect all data into the lists\n",
    "        names     += [name]\n",
    "        dates     += [date_year]\n",
    "        mmaxs     += [mmax]\n",
    "        hosts     += [host]\n",
    "        zs        += [z]\n",
    "        types     += [tipe]\n",
    "        phots     += [phot]\n",
    "        specs     += [spec]\n",
    "        ras_deg   += [ra_deg]\n",
    "        decs_deg  += [dec_deg]\n",
    "        ras_diff  += [ra_diff]\n",
    "        decs_diff += [dec_diff]\n",
    "\n",
    "    return(names, dates, mmaxs, hosts, zs, types, phots, specs, ras_deg, decs_deg, ras_diff, decs_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, dates, mmaxs, hosts, zs, types, phots, specs, ras_deg, decs_deg, ras_diff, decs_diff = clean_catalog(full_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_osc_dict = {\n",
    "    'NAME'    : names,\n",
    "    'DATE'    : dates,\n",
    "    'HOST'    : hosts,\n",
    "    'RA'      : ras_deg,\n",
    "    'RA_diff' : ras_diff,\n",
    "    'DEC'     : decs_deg,\n",
    "    'DEC_diff': decs_diff,\n",
    "    'TYPE'    : types,\n",
    "    'mmax'    : mmaxs,\n",
    "    'z'       : zs,\n",
    "    'PHOT'    : phots,\n",
    "    'SPEC'    : specs,\n",
    "}\n",
    "\n",
    "clean_osc = pd.DataFrame(clean_osc_dict, columns=clean_osc_dict.keys()) # columns arg will keep order\n",
    "clean_osc = clean_osc[clean_osc['TYPE'].notnull()]\n",
    "clean_osc.to_csv('samples/clean_osc_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop SN types that are not Ia, II, and Ibc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_osc = pd.read_csv('samples/clean_osc_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow down the parent table by SN type\n",
    "bitmask = (((clean_osc.TYPE == 'Ia') |\n",
    "            (clean_osc.TYPE == 'II') |\n",
    "            (clean_osc.TYPE == 'II P') |\n",
    "            (clean_osc.TYPE == 'II L') |\n",
    "            (clean_osc.TYPE == 'II-P/L') | # just 1 SN\n",
    "            (clean_osc.TYPE == 'IIb') |\n",
    "            (clean_osc.TYPE == 'Ib') |\n",
    "            (clean_osc.TYPE == 'Ic') |\n",
    "            (clean_osc.TYPE == 'Ibc')|\n",
    "            (clean_osc.TYPE == 'Ib/c')))\n",
    "clean_osc = clean_osc[bitmask]\n",
    "\n",
    "clean_osc = clean_osc[clean_osc['TYPE'].notnull()]\n",
    "clean_osc.to_csv('samples/clean_osc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_osc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-match The Open Supernova Catalog with the original z0MGS images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_in_image(names, dates, hosts, ras, ras_diff, decs, decs_diff, types, zs, phots, specs, wcs):\n",
    "\n",
    "    coords_arr = np.column_stack((ras, decs))\n",
    "    world_coords_all = wcs.wcs_world2pix(coords_arr, 0)\n",
    "\n",
    "    world_x = world_coords_all[:,0]\n",
    "    world_y = world_coords_all[:,1]\n",
    "\n",
    "    # use world coordinates of all SNe to see if any fall in image (our version of footprint_contains)\n",
    "    naxis = wcs._naxis # size of image\n",
    "    is_in_x = (world_x >= 0) & (world_x <= naxis[0]-1) # because of 0-indexing\n",
    "    is_in_y = (world_y >= 0) & (world_y <= naxis[1]-1)\n",
    "             \n",
    "    # get the name, ra, and dec of the SNe that fall in image\n",
    "    name_in_image       = np.array(names)[is_in_x & is_in_y]\n",
    "    date_in_image       = np.array(dates)[is_in_x & is_in_y]\n",
    "    host_in_image       = np.array(hosts)[is_in_x & is_in_y]\n",
    "    ra_in_image         = np.array(ras)[is_in_x & is_in_y]\n",
    "    dec_in_image        = np.array(decs)[is_in_x & is_in_y]\n",
    "    ra_diff_in_image    = np.array(ras_diff)[is_in_x & is_in_y]\n",
    "    dec_diff_in_image   = np.array(decs_diff)[is_in_x & is_in_y]\n",
    "    types_in_image      = np.array(types)[is_in_x & is_in_y]\n",
    "    z_in_image          = np.array(zs)[is_in_x & is_in_y]\n",
    "    phot_in_image       = np.array(phots)[is_in_x & is_in_y]\n",
    "    spec_in_image       = np.array(specs)[is_in_x & is_in_y]\n",
    "    \n",
    "    # to be used in conditional statement\n",
    "    x_coord        = np.array(world_x)[is_in_x & is_in_y]\n",
    "    y_coord        = np.array(world_y)[is_in_x & is_in_y]\n",
    "\n",
    "\n",
    "    return(name_in_image, date_in_image, host_in_image, ra_in_image, ra_diff_in_image, dec_in_image, dec_diff_in_image, types_in_image, z_in_image, phot_in_image, spec_in_image, x_coord, y_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(name_in_image, date_in_image, img_name, host_in_image, ra_in_image, ra_diff_in_image, \n",
    "                dec_in_image, dec_diff_in_image, types_in_image, z_in_image, phot_in_image, spec_in_image):\n",
    "    t = Table()\n",
    "    t['NAME']     = name_in_image\n",
    "    t['DATE']     = date_in_image\n",
    "    t['PGC']      = img_name\n",
    "    t['HOST']     = host_in_image\n",
    "    t['RA']       = ra_in_image\n",
    "    t['RA_diff']  = ra_diff_in_image\n",
    "    t['DEC']      = dec_in_image\n",
    "    t['DEC_diff'] = dec_diff_in_image\n",
    "    t['TYPE']     = types_in_image\n",
    "    t['z']        = z_in_image\n",
    "    t['PHOT']     = phot_in_image\n",
    "    t['SPEC']     = spec_in_image\n",
    "    return(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now just use clean_osc.csv, which only has types Ia, II, and Ibc. (no IIn, Ia-Pec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# osc = pd.read_csv('samples/clean_osc.csv')\n",
    "osc = pd.read_csv('samples/clean_osc_full.csv')\n",
    "names     = osc['NAME']\n",
    "dates     = osc['DATE']\n",
    "hosts     = osc['HOST']\n",
    "ras       = osc['RA']\n",
    "ras_diff  = osc['RA_diff']\n",
    "decs      = osc['DEC']\n",
    "decs_diff = osc['DEC_diff']\n",
    "types     = osc['TYPE']\n",
    "zs        = osc['z']\n",
    "phots     = osc['PHOT']\n",
    "specs     = osc['SPEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_list = []\n",
    "\n",
    "count = 0\n",
    "bar = ProgressBar(os.listdir('/data/kant/0/leroy.42/allsky/delivery/'), ipython_widget=True)\n",
    "for filename in os.listdir('/data/kant/0/leroy.42/allsky/delivery/'):\n",
    "    if filename.endswith('_w1_gauss7p5.fits'):\n",
    "\n",
    "        img_dir = ('/data/kant/0/leroy.42/allsky/delivery/' + filename)\n",
    "        img_name = filename.split('_')[0]\n",
    "        \n",
    "        if img_name == 'PGC2557':\n",
    "            bar.update()\n",
    "            continue\n",
    "        if img_name == 'PGC5818':\n",
    "            bar.update()\n",
    "            continue\n",
    "        \n",
    "        hdulist = pyfits.open(img_dir)\n",
    "        wcs = WCS(hdulist[0].header)\n",
    "\n",
    "        # call check_in_image\n",
    "        try:\n",
    "            name_in_image, date_in_image, host_in_image, ra_in_image, ra_diff_in_image, dec_in_image, dec_diff_in_image, types_in_image, z_in_image, phot_in_image, spec_in_image, x_coord, y_coord = check_in_image(names, dates, hosts, ras, ras_diff, decs, decs_diff, types, zs, phots, specs, wcs)\n",
    "            if len(x_coord) == 0:\n",
    "                bar.update()\n",
    "                continue\n",
    "            else:\n",
    "                t = build_table(name_in_image, date_in_image, img_name, host_in_image, ra_in_image, ra_diff_in_image, dec_in_image, dec_diff_in_image, types_in_image, z_in_image, phot_in_image, spec_in_image)\n",
    "                table_list += [t]\n",
    "        except:\n",
    "            bar.update()\n",
    "            continue\n",
    "        \n",
    "        hdulist.close()\n",
    "    bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatch = vstack(table_list)\n",
    "xmatch_df = xmatch.to_pandas()\n",
    "# xmatch_df.to_csv('samples/xmatch.csv', index=False)\n",
    "xmatch_df.to_csv('samples/xmatch_full.csv', index=False)\n",
    "xmatch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add columns that have host galaxy redshift, the difference between host galaxy redshift and SN redshift, and a believable vs. doubtful sample.\n",
    "We want to add a column for host galaxy redshift and the difference between host galaxy redshift and SN redshift. This will allow us to make cuts based on redshift to ensure that SN are being placed in their correct host galaxies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add galbase information to xmatch.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xmatch = pd.read_csv('samples/xmatch.csv')\n",
    "xmatch = pd.read_csv('samples/xmatch_full.csv')\n",
    "galbase  = pd.read_csv('samples/galbase_info.csv')\n",
    "\n",
    "galbase_dict = {\n",
    "    'PGC'    : galbase['PGC'],\n",
    "    'RA_GAL' : galbase['RA'],\n",
    "    'DEC_GAL': galbase['DEC'],\n",
    "    'INCL'   : galbase['INCL'],\n",
    "    'PA'     : galbase['PA'],\n",
    "    'R25'    : galbase['R25'],\n",
    "    'VEL'    : galbase['VEL_REC'],\n",
    "    'T'      : galbase['T']\n",
    "}\n",
    "\n",
    "galbase_df = pd.DataFrame(galbase_dict)\n",
    "xmatch = xmatch.merge(galbase_df, on=['PGC'])\n",
    "xmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_avg = []\n",
    "for index, row in xmatch.iterrows():\n",
    "    zs   = str(row['z']).split(',')\n",
    "    vel  = row['VEL']\n",
    "    \n",
    "    # turn every z value per row into a float\n",
    "    item_floats = []\n",
    "    for val in zs:\n",
    "        val = float(val)\n",
    "        item_floats += [val]\n",
    "        \n",
    "    # take the median of all the z values together per row    \n",
    "    z_avg = np.median(item_floats)\n",
    "    zs_avg += [z_avg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that converts recessional velocity to redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert recessional velocity to redshift\n",
    "def vel_to_z(v):\n",
    "    return(v / (3*10**5))\n",
    "\n",
    "xmatch['z']     = zs_avg\n",
    "xmatch['z_gal'] = xmatch['VEL'].apply(vel_to_z)\n",
    "\n",
    "# take the difference between redshift of SNe and redshift of their supposed galaxies\n",
    "xmatch['z_diff'] = abs(xmatch['z'] - xmatch['z_gal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designate a believable sample and a doubtful sample based on if redshift and host galaxy information are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_type = []\n",
    "\n",
    "for index, row in xmatch.iterrows():\n",
    "    z    = str(row['z'])\n",
    "    host = str(row['HOST'])\n",
    "    \n",
    "    if (z == 'nan') and (host == 'nan'):\n",
    "        sample_type += ['D']\n",
    "    else:\n",
    "        sample_type += ['B']\n",
    "\n",
    "xmatch['SAMPLE'] = sample_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatch = xmatch[xmatch['SAMPLE'] == 'B']\n",
    "xmatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates occur when there are more than one galaxy in a given field (or, in a lesser case, when there are multiple entries for one SN in the OSC). We need to get rid of duplicates by matching each SN with its correct PGC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatch_dup = xmatch[xmatch.duplicated(subset='NAME', keep=False) == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the correct PGCs with the correct SNe, we will first use 2*R25 and draw an ellipse over each galaxy. We will see if the SNe falls within this ellipse. Output this to a file. This is our potential SNe sample with no cuts based on redshift, type, or getting rid of duplicates (though duplicates are indicated).\n",
    "\n",
    "Get rid of everything lacking a position angle, incl, or has an incl > 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatch = xmatch[xmatch['PA'].isnull() | xmatch['INCL'].isnull() == False]\n",
    "xmatch = xmatch[xmatch['INCL'] <= 60.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep everything within 2*R25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipse(delta_ra, delta_dec, pa, incl, r25_rad):\n",
    "    part1 = ((-delta_ra*math.cos(np.radians(pa+90)) + delta_dec*math.sin(np.radians(pa+90)))**2)/(2*r25_rad**2)\n",
    "    part2 = ((-delta_ra*math.sin(np.radians(pa+90)) - delta_dec*math.cos(np.radians(pa+90)))**2)/(2*r25_rad*math.cos(np.radians(incl)))**2\n",
    "    return(part1, part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_gal(samp):\n",
    "    in_galaxy = []\n",
    "\n",
    "    bar = ProgressBar(len(samp), ipython_widget=True)\n",
    "    for index, row in samp.iterrows():\n",
    "\n",
    "            bar.update()\n",
    "\n",
    "            # grab ra, dec, R25, inclination, and position angle; get ra, dec, and R25 in radians\n",
    "            ra_gal  = np.radians(row['RA_GAL'])\n",
    "            dec_gal = np.radians(row['DEC_GAL'])\n",
    "            r25     = np.radians(row['R25'])\n",
    "            incl    = row['INCL']\n",
    "            pa      = row['PA']\n",
    "\n",
    "            # grab RA and DEC of SN; get them in radians\n",
    "            ra_sn  = np.radians(row['RA'])\n",
    "            dec_sn = np.radians(row['DEC'])\n",
    "\n",
    "            # take difference in RA and DEC of galaxy and RA and DEC of SNe\n",
    "            delta_ra = ra_gal - ra_sn\n",
    "            delta_dec = dec_gal - dec_sn\n",
    "\n",
    "#             part1, part2 = ellipse(delta_ra, delta_dec, pa, incl, r25)\n",
    "            part1, part2 = ellipse(delta_ra, delta_dec, pa, incl, r25) # re-run but for those within r25\n",
    "            e = part1 + part2\n",
    "\n",
    "            # compare distance with eqtn for inclined, oblique ellipse\n",
    "            if e <= 1:\n",
    "                in_galaxy += [1]\n",
    "            else:\n",
    "                in_galaxy += [0]\n",
    "    return(in_galaxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_galaxy = in_gal(xmatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatch['IN_2R25'] = in_galaxy\n",
    "# xmatch.to_csv('samples/potential_sample.csv', index=False)\n",
    "xmatch.to_csv('samples/potential_sample_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop things that are outside of 2*R25 or have a difference in redshift greater than 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential_sample = pd.read_csv('samples/potential_sample.csv')\n",
    "potential_sample = pd.read_csv('samples/potential_sample_full.csv')\n",
    "potential_sample = potential_sample[potential_sample['IN_2R25'] == 1]\n",
    "potential_sample = potential_sample[potential_sample['z_diff'] <= 0.002]\n",
    "potential_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates. (Our strict constraints seem to have weeded out any potential duplicates. If in the future there are duplicates, see sne_sample in the old directory for how to deal with these.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_sample[potential_sample.duplicated(subset='NAME', keep=False) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_sample.to_csv('samples/sne_sample_full.csv', index=False)\n",
    "# potential_sample.to_csv('samples/sne_sample.csv', index=False)\n",
    "potential_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
